{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchaudio\n",
    "!pip install -e .\n",
    "!pip install datasets\n",
    "!pip install huggingface_hub\n",
    "!pip install wandb\n",
    "!pip install cached_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data_perm/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.687 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word segmentation module jieba initialized.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "sys.path.append('/data_perm/F5-TTS_custom/src/')\n",
    "\n",
    "from cached_path import cached_path\n",
    "from f5_tts.model import CFM, UNetT, DiT, Trainer\n",
    "from f5_tts.model.utils import get_tokenizer\n",
    "from f5_tts.model.dataset import load_dataset\n",
    "from importlib.resources import files\n",
    "import csv\n",
    "import json\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "local_dir = \"/data_perm/\"\n",
    "filename = \"all_mp3_data.zip\"\n",
    "\n",
    "hf_hub_download(repo_id=\"MonoraAI/test\", filename=filename, local_dir=local_dir, repo_type=\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -qq  /data_perm/all_mp3_data.zip -d /data_perm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.668 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Word segmentation module jieba initialized.\n",
      "\n",
      "\n",
      "Saving to /data_perm/F5-TTS_custom/data/turkish_data_char ...\n",
      "Writing to raw.arrow ...: 100%|█████████| 21905/21905 [00:09<00:00, 2400.35it/s]\n",
      "\n",
      "For turkish_data_char, sample count: 21905\n",
      "For turkish_data_char, vocab size is: 113\n",
      "For turkish_data_char, total 94.01 hours\n"
     ]
    }
   ],
   "source": [
    "!python /data_perm/F5-TTS_custom/src/f5_tts/train/datasets/prepare_csv_wavs.py \\\n",
    "    /data_perm/all_mp3_data/ \\\n",
    "    /data_perm/F5-TTS_custom/data/turkish_data_char/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The above cell proces a .arrow type file. To view its content use the below cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data_perm/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['audio_path', 'text', 'duration'],\n",
      "    num_rows: 21905\n",
      "})\n",
      "{'audio_path': '/data_perm/all_mp3_data/wavs/CHP_sandıklara_sahip_çıkabilecek_mi_CHP_Ataşehir_Bld_Bşk_Adayı_Onursal_Adıgüzel__Fatih_Altaylı_0_1.mp3', 'text': ['S', 'o', 'h', 'b', 'e', 't', ' ', 'e', 't', 'm', 'e', ' ', 'v', 'e', ' ', 'y', 'a', 'p', 'm', 'a', 'y', 'ı', ' ', 'p', 'l', 'a', 'n', 'l', 'a', 'd', 'ı', ' ', 'k', 'l', 'a', 'r', 'ı', 'n', 'ı', ' ', 'ö', 'ğ', 'r', 'e', 'n', 'm', 'e', ' ', 'a', 'm', 'a', 'ç', 'l', 'ı', '.', ' ', 'A', 'm', 'a', ' ', 't', 'a', 'b', 'i', 'i', ' ', 'g', 'ö', 'r', 'e', 'v', ' ', 'b', 'a', 'ş', 'ı', ' ', 'n', 'd', 'a', 'k', 'i', ' ', 'b', 'e', 'l', 'e', 'd', 'i', 'y', 'e', ' ', 'b', 'a', 'ş', 'k', 'a', 'n', 'l', 'a', 'r', 'ı', ' ', 'y', 'l', 'a', ' ', 'a', 'd', 'a', 'y', 'l', 'a', 'r', ' ', 'a', 'r', 'a', 's', 'ı', ' ', 'n', 'd', 'a', ' ', 'd', 'a', ' ', 'e', 'l', 'b', 'e', 't', 't', 'e', ' ', 'b', 'i', 'r', ' ', 'a', 'y', 'r', 'ı', 'm', ' ', 'o', 'l', 'u', 'y', 'o', 'r', '.', ' ', 'G', 'ö', 'r', 'e', 'v', ' ', 'b', 'a', 'ş', 'ı', ' ', 'n', 'd', 'a', 'k', 'i', 'l', 'e', 'r', 'i', 'n', ' ', 'd', 'a', 'h', 'a', ' ', 'ç', 'o', 'k', ' ', 'y', 'a', 'p', 't', 'ı', ' ', 'k', 'l', 'a', 'r', 'ı', 'n', 'ı', ' ', 'a', 'd', 'a', 'y', 'l', 'a', 'r', 'a', ' ', 'i', 's', 'e', ' ', 'y', 'a', 'p', 'm', 'a', 'y', 'ı', ' ', 'p', 'l', 'a', 'n', 'l', 'a', 'd', 'ı', ' ', 'k', 'l', 'a', 'r', 'ı', 'n', 'ı', ' ', 's', 'o', 'r', 'm', 'a', 'y', 'ı', ' ', 't', 'e', 'r', 'c', 'i', 'h', ' ', 'e', 'd', 'i', 'y', 'o', 'r', 'u', 'z', '.'], 'duration': 14.24}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Path to the Arrow file\n",
    "arrow_file_path = \"/data_perm/F5-TTS_custom/data/turkish_data_char/raw.arrow\"\n",
    "\n",
    "# Load the dataset\n",
    "dataset = Dataset.from_file(arrow_file_path)\n",
    "\n",
    "# Print dataset info and a few samples\n",
    "print(dataset)\n",
    "print(dataset[1])  # View the first sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a4058b2c4cf4220c074494541c001ce879794d05\n"
     ]
    }
   ],
   "source": [
    "wandb_project = \"F5_TTS_Turkish2\"\n",
    "wandb_run_name = \"F5_TTS_Turkish_Run\"\n",
    "wandb_resume_id = None\n",
    "import wandb\n",
    "os.environ[\"WANDB_API_KEY\"] = \"a4058b2c4cf4220c074494541c001ce879794d05\"\n",
    "print(wandb.api.api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- Dataset Settings --------------------------- #\n",
    "target_sample_rate = 24000\n",
    "n_mel_channels = 100\n",
    "hop_length = 256\n",
    "win_length = 1024\n",
    "n_fft = 1024\n",
    "mel_spec_type = \"vocos\"  # 'vocos' or 'bigvgan'\n",
    "\n",
    "# -------------------------- Argument Variables ------------------------- #\n",
    "dataset_name = \"turkish_data\"\n",
    "learning_rate = 1e-5\n",
    "batch_size_per_gpu = 6400\n",
    "batch_size_type = \"frame\"\n",
    "max_samples = 64\n",
    "grad_accumulation_steps = 1\n",
    "max_grad_norm = 1.0\n",
    "epochs = 20\n",
    "num_warmup_updates = 300\n",
    "save_per_updates = 2000\n",
    "keep_last_n_checkpoints = 4\n",
    "last_per_updates = 2000\n",
    "finetune = True\n",
    "pretrain = None\n",
    "tokenizer = \"char\"\n",
    "tokenizer_path = None\n",
    "log_samples = False\n",
    "logger = \"wandb\"\n",
    "bnb_optimizer = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"/data_perm/F5-TTS_custom/ckpts/turkish/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "vocab : 2554\n",
      "\n",
      "vocoder : vocos\n"
     ]
    }
   ],
   "source": [
    "# -------------------------- Training Settings -------------------------- #\n",
    "# Model parameters based on experiment name\n",
    "\n",
    "model_cls = DiT\n",
    "model_cfg = dict(dim=1024, depth=22, heads=16, ff_mult=2, text_dim=512, conv_layers=4)\n",
    "\n",
    "vocab_char_map, vocab_size = get_tokenizer(\"/data_perm/F5-TTS_custom/ckpts/turkish/vocab.txt\", \"custom\")\n",
    "\n",
    "print(\"\\nvocab :\", vocab_size)\n",
    "print(\"\\nvocoder :\", mel_spec_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmonoraai\u001b[0m (\u001b[33mmonoraai-monora\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data_perm/F5-TTS_custom/wandb/run-20250131_004107-ebfp76cr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/monoraai-monora/F5_TTS_Turkish2/runs/ebfp76cr' target=\"_blank\">F5_TTS_Turkish_Run</a></strong> to <a href='https://wandb.ai/monoraai-monora/F5_TTS_Turkish2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/monoraai-monora/F5_TTS_Turkish2' target=\"_blank\">https://wandb.ai/monoraai-monora/F5_TTS_Turkish2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/monoraai-monora/F5_TTS_Turkish2/runs/ebfp76cr' target=\"_blank\">https://wandb.ai/monoraai-monora/F5_TTS_Turkish2/runs/ebfp76cr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using logger: wandb\n",
      "Loading dataset ...\n"
     ]
    }
   ],
   "source": [
    "mel_spec_kwargs = dict(\n",
    "    n_fft=n_fft,\n",
    "    hop_length=hop_length,\n",
    "    win_length=win_length,\n",
    "    n_mel_channels=n_mel_channels,\n",
    "    target_sample_rate=target_sample_rate,\n",
    "    mel_spec_type=mel_spec_type,\n",
    ")\n",
    "\n",
    "model = CFM(\n",
    "    transformer=model_cls(**model_cfg, text_num_embeds=vocab_size, mel_dim=n_mel_channels),\n",
    "    mel_spec_kwargs=mel_spec_kwargs,\n",
    "    vocab_char_map=vocab_char_map,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    epochs,\n",
    "    learning_rate,\n",
    "    num_warmup_updates=num_warmup_updates,\n",
    "    save_per_updates=save_per_updates,\n",
    "    keep_last_n_checkpoints=keep_last_n_checkpoints,\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    batch_size=batch_size_per_gpu,\n",
    "    batch_size_type=batch_size_type,\n",
    "    max_samples=max_samples,\n",
    "    grad_accumulation_steps=grad_accumulation_steps,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    logger=\"wandb\",\n",
    "    wandb_project= wandb_project,\n",
    "    wandb_run_name= wandb_run_name,\n",
    "    wandb_resume_id=wandb_resume_id,\n",
    "    log_samples=log_samples,\n",
    "    last_per_updates=last_per_updates,\n",
    "    bnb_optimizer=bnb_optimizer,\n",
    ")\n",
    "\n",
    "train_dataset = load_dataset(dataset_name, tokenizer, dataset_type=\"CustomDataset\", mel_spec_kwargs=mel_spec_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sorting with sampler... if slow, check whether dataset is provided with duration: 100%|██████████| 21905/21905 [00:00<00:00, 1591840.00it/s]\n",
      "Creating dynamic batches with 6400 audio frames per gpu: 100%|██████████| 21905/21905 [00:00<00:00, 1919125.81it/s]\n",
      "/data_perm/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved last checkpoint at update 4000\n"
     ]
    }
   ],
   "source": [
    "trainer.train(train_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
